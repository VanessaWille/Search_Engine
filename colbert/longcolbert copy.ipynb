{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Context Colbert\n",
    "\n",
    "Vamos criar uma aplicação do Vespa com Long Context Colbert, como pode ser visto [neste exemplo](https://pyvespa.readthedocs.io/en/latest/examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify if the gpu is available\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Primeiro, vamos importar as bibliotecas necessárias para criar pacotes do Vespa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import (\n",
    "    ApplicationPackage,\n",
    "    Component,\n",
    "    Parameter,\n",
    "    Field,\n",
    "    HNSW,\n",
    "    RankProfile,\n",
    "    Function,\n",
    "    FirstPhaseRanking,\n",
    "    SecondPhaseRanking,\n",
    "    FieldSet,\n",
    "    DocumentSummary,\n",
    "    Summary,\n",
    ")\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "from vespa.package import Schema, Document, Field, FieldSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar se o Vespa está instalado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "  vespa [flags]\n",
      "  vespa [command]\n",
      "\n",
      "Available Commands:\n",
      "  activate    Activate (deploy) a previously prepared application package\n",
      "  auth        Manage Vespa Cloud credentials\n",
      "  clone       Create files and directory structure from a Vespa sample application\n",
      "  completion  Generate the autocompletion script for the specified shell\n",
      "  config      Manage persistent values for global flags\n",
      "  curl        Access Vespa directly using curl\n",
      "  deploy      Deploy (prepare and activate) an application package\n",
      "  destroy     Remove a deployed Vespa application and its data\n",
      "  document    Issue a single document operation to Vespa\n",
      "  feed        Feed multiple document operations to Vespa\n",
      "  fetch       Download a deployed application package\n",
      "  help        Help about any command\n",
      "  log         Show the Vespa log\n",
      "  prepare     Prepare an application package for activation\n",
      "  prod        Deploy an application package to production in Vespa Cloud\n",
      "  query       Issue a query to Vespa\n",
      "  status      Show Vespa endpoints and status\n",
      "  test        Run a test suite, or a single test\n",
      "  version     Show current CLI version and check for updates\n",
      "  visit       Retrieve and print all documents from Vespa\n",
      "\n",
      "Flags:\n",
      "  -a, --application string   The application to use (cloud only)\n",
      "  -C, --cluster string       The container cluster to use. This is only required for applications with multiple clusters\n",
      "  -c, --color string         Whether to use colors in output. Must be \"auto\", \"never\", or \"always\" (default \"auto\")\n",
      "  -h, --help                 help for vespa\n",
      "  -i, --instance string      The instance of the application to use (cloud only)\n",
      "  -q, --quiet                Print only errors\n",
      "  -t, --target string        The target platform to use. Must be \"local\", \"cloud\", \"hosted\" or an URL (default \"local\")\n",
      "  -z, --zone string          The zone to use. This defaults to a dev zone (cloud only)\n",
      "\n",
      "Use \"vespa [command] --help\" for more information about a command.\n",
      "\n",
      "Error: unknown flag: --version\n"
     ]
    }
   ],
   "source": [
    "!vespa --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do aplicativo Vespa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar o pacote do aplicativo Vespa, com os componentes `e5` e `colbert`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage, Component, Parameter\n",
    "\n",
    "vespa_app_name = \"findmypasta\"\n",
    "app_package = ApplicationPackage(\n",
    "    name=vespa_app_name,\n",
    "    components=[\n",
    "        Component(\n",
    "            id=\"e5\",\n",
    "            type=\"hugging-face-embedder\",\n",
    "            parameters=[\n",
    "                Parameter(\n",
    "                    name=\"transformer-model\",\n",
    "                    args={\n",
    "                        \"url\": \"https://huggingface.co/intfloat/e5-small-v2/resolve/main/model.onnx\"\n",
    "                    },\n",
    "                ),\n",
    "                Parameter(\n",
    "                    name=\"tokenizer-model\",\n",
    "                    args={\n",
    "                        \"url\": \"https://huggingface.co/intfloat/e5-small-v2/raw/main/tokenizer.json\"\n",
    "                    },\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "        Component(\n",
    "            id=\"colbert\",\n",
    "            type=\"colbert-embedder\",\n",
    "            parameters=[\n",
    "                Parameter(\n",
    "                    name=\"transformer-model\",\n",
    "                    args={\n",
    "                        \"url\": \"https://huggingface.co/colbert-ir/colbertv2.0/resolve/main/model.onnx\"\n",
    "                    },\n",
    "                ),\n",
    "                Parameter(\n",
    "                    name=\"tokenizer-model\",\n",
    "                    args={\n",
    "                        \"url\": \"https://huggingface.co/colbert-ir/colbertv2.0/raw/main/tokenizer.json\"\n",
    "                    },\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar o *Schema* com os campos da nossa receita, o `embedding` e o `colbert`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_package.schema.add_fields(\n",
    "    Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),\n",
    "    Field(\n",
    "        name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n",
    "    ),\n",
    "    # Field(\n",
    "    #     name=\"description\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n",
    "    # ),\n",
    "    # Field(\n",
    "    #     name=\"minutes\",\n",
    "    #     type=\"string\",\n",
    "    #     indexing=[\"summary\"],\n",
    "    # ),\n",
    "    # Field(\n",
    "    #     name=\"n_steps\",\n",
    "    #     type=\"string\",\n",
    "    #     indexing=[\"attribute\", \"summary\"],\n",
    "    # ),\n",
    "    # Field(\n",
    "    #     name=\"n_ingredients\",\n",
    "    #     type=\"string\",\n",
    "    #     indexing=[\"attribute\", \"summary\"],\n",
    "    # ),\n",
    "    # Field(\n",
    "    #     name=\"submitted\",\n",
    "    #     type=\"string\",\n",
    "    #     indexing=[\"attribute\", \"summary\"],\n",
    "    # ),\n",
    "    Field(\n",
    "        name=\"body\",\n",
    "        type=\"string\", \n",
    "        indexing=[\"index\", \"summary\"],\n",
    "        index=\"enable-bm25\",\n",
    "        bolding=True\n",
    "    ),\n",
    "    Field(\n",
    "        name = \"body_split\",\n",
    "        type = \"array<string>\",\n",
    "        indexing = [\"index\", \"summary\"],\n",
    "        index = \"enable-bm25\",\n",
    "        bolding = True,\n",
    "    ),\n",
    "    # Field(\n",
    "    #     name=\"tags\",\n",
    "    #     type=\"array<string>\",\n",
    "    #     indexing=[\"index\", \"summary\"],\n",
    "    #     index=\"enable-bm25\",\n",
    "    #     bolding=True,\n",
    "    # ),\n",
    "    Field(\n",
    "        name=\"steps\",\n",
    "        type=\"array<string>\",\n",
    "        indexing=[\"index\", \"summary\"],\n",
    "        index=\"enable-bm25\",\n",
    "        bolding=True,\n",
    "    ),\n",
    "    Field(\n",
    "        name=\"ingredients\",\n",
    "        type=\"array<string>\",\n",
    "        indexing=[\"index\", \"summary\"],\n",
    "        index=\"enable-bm25\",\n",
    "        bolding=True,\n",
    "    ),\n",
    "    #Field(\n",
    "    #    name=\"colbert\",\n",
    "    #    type=\"tensor<int8>(token{},v[16])\",\n",
    "    #    indexing=[\"attribute\", \"summary\", \"index\"],\n",
    "    #    attribute=[\"distance-metric:hamming\"],\n",
    "    #)\n",
    "    Field(\n",
    "    name=\"embedding_body_split\",\n",
    "    type=\"tensor<bfloat16>(body_split{}, x[384])\",\n",
    "    indexing=[\n",
    "        \"input body_split\",\n",
    "        \"embed e5\",\n",
    "        \"attribute\",\n",
    "    ],\n",
    "    attribute=[\"distance-metric: angular\"],\n",
    "    is_document_field=False,\n",
    "    ),\n",
    "    Field(\n",
    "    name=\"colbert_body_split\",\n",
    "    type=\"tensor<int8>(body_split{}, token{}, v[16])\",\n",
    "    indexing=[\"input body_split\", \"embed colbert body_split\", \"attribute\"],\n",
    "    is_document_field=False,\n",
    "    ),\n",
    "    Field(\n",
    "    name=\"embedding_steps\",\n",
    "    type=\"tensor<bfloat16>(steps{}, x[384])\",\n",
    "    indexing=[\n",
    "        \"input steps\",\n",
    "        \"embed e5\",\n",
    "        \"attribute\",\n",
    "    ],\n",
    "    attribute=[\"distance-metric: angular\"],\n",
    "    is_document_field=False,\n",
    "    ),\n",
    "    Field(\n",
    "    name=\"colbert_steps\",\n",
    "    type=\"tensor<int8>(steps{}, token{}, v[16])\",\n",
    "    indexing=[\"input steps\", \"embed colbert steps\", \"attribute\"],\n",
    "    is_document_field=False,\n",
    "    ),\n",
    "    Field(\n",
    "    name=\"embedding_ingredients\",\n",
    "    type=\"tensor<bfloat16>(ingredients{}, x[384])\",\n",
    "    indexing=[\n",
    "        \"input ingredients\",\n",
    "        \"embed e5\",\n",
    "        \"attribute\",\n",
    "    ],\n",
    "    attribute=[\"distance-metric: angular\"],\n",
    "    is_document_field=False,\n",
    "    ),\n",
    "    Field(\n",
    "    name=\"colbert_ingredients\",\n",
    "    type=\"tensor<int8>(ingredients{}, token{}, v[16])\",\n",
    "    indexing=[\"input ingredients\", \"embed colbert ingredients\", \"attribute\"],\n",
    "    is_document_field=False,\n",
    "    )\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar o *RankProfile* com o [Colbert Context-Level](https://blog.vespa.ai/announcing-long-context-colbert-in-vespa/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_max_body_split = RankProfile(\n",
    "    name=\"colbert_max_body_split\",\n",
    "    inputs=[\n",
    "        (\"query(q)\", \"tensor<float>(x[384])\"),\n",
    "        (\"query(qt)\", \"tensor<float>(querytoken{}, v[128])\"),\n",
    "    ],\n",
    "    functions=[\n",
    "        Function(name=\"cos_sim_body_split\", expression=\"closeness(field, embedding_body_split)\"),\n",
    "        Function(\n",
    "            name=\"max_sim_per_context_body_split\",\n",
    "            expression=\"\"\"\n",
    "                sum(\n",
    "                    reduce(\n",
    "                        sum(\n",
    "                            query(qt) * unpack_bits(attribute(colbert_body_split)), v\n",
    "                        ),\n",
    "                        max, token\n",
    "                    ),\n",
    "                    querytoken\n",
    "                )\n",
    "            \"\"\",\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"max_sim_body_split\", expression=\"reduce(max_sim_per_context_body_split, max, body_split)\"\n",
    "            \n",
    "        ),\n",
    "    ],\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_body_split\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"max_sim_body_split\"),\n",
    "    match_features=[\"cos_sim_body_split\", \"max_sim_body_split\", \"max_sim_per_context_body_split\"],\n",
    ")\n",
    "\n",
    "colbert_avg_body_split = RankProfile(\n",
    "    name=\"colbert_avg_body_split\",\n",
    "    inputs=[\n",
    "        (\"query(q)\", \"tensor<float>(x[384])\"),\n",
    "        (\"query(qt)\", \"tensor<float>(querytoken{}, v[128])\"),\n",
    "    ],\n",
    "    functions=[\n",
    "        Function(name=\"cos_sim_body_split\", expression=\"closeness(field, embedding_body_split)\"),\n",
    "        Function(\n",
    "            name=\"max_sim_per_context_body_split\",\n",
    "            expression=\"\"\"\n",
    "                sum(\n",
    "                    reduce(\n",
    "                        sum(\n",
    "                            query(qt) * unpack_bits(attribute(colbert_body_split)), v\n",
    "                        ),\n",
    "                        max, token\n",
    "                    ),\n",
    "                    querytoken\n",
    "                )\n",
    "            \"\"\",\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"avg_sim_body_split\", expression=\"reduce(max_sim_per_context_body_split, avg, body_split)\"\n",
    "        ),\n",
    "    ],\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_body_split\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"avg_sim_body_split\"),\n",
    "    match_features=[\"cos_sim_body_split\", \"avg_sim_body_split\", \"max_sim_per_context_body_split\"],\n",
    ")\n",
    "\n",
    "colbert_max_steps= RankProfile(\n",
    "    name=\"colbert_max_steps\",\n",
    "    inputs=[\n",
    "        (\"query(q)\", \"tensor<float>(x[384])\"),\n",
    "        (\"query(qt)\", \"tensor<float>(querytoken{}, v[128])\"),\n",
    "    ],\n",
    "    functions=[\n",
    "        Function(name=\"cos_sim_steps\", expression=\"closeness(field, embedding_steps)\"),\n",
    "        Function(\n",
    "            name=\"max_sim_per_context_steps\",\n",
    "            expression=\"\"\"\n",
    "                sum(\n",
    "                    reduce(\n",
    "                        sum(\n",
    "                            query(qt) * unpack_bits(attribute(colbert_steps)), v\n",
    "                        ),\n",
    "                        max, token\n",
    "                    ),\n",
    "                    querytoken\n",
    "                )\n",
    "            \"\"\",\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"max_sim_steps\", expression=\"reduce(max_sim_per_context_steps, max, steps)\"\n",
    "            \n",
    "        ),\n",
    "    ],\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_steps\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"max_sim_steps\"),\n",
    "    match_features=[\"cos_sim_steps\", \"max_sim_steps\", \"max_sim_per_context_steps\"],\n",
    ")\n",
    "\n",
    "colbert_avg_steps = RankProfile(\n",
    "    name=\"colbert_avg_steps\",\n",
    "    inputs=[\n",
    "        (\"query(q)\", \"tensor<float>(x[384])\"),\n",
    "        (\"query(qt)\", \"tensor<float>(querytoken{}, v[128])\"),\n",
    "    ],\n",
    "    functions=[\n",
    "        Function(name=\"cos_sim_steps\", expression=\"closeness(field, embedding_steps)\"),\n",
    "        Function(\n",
    "            name=\"max_sim_per_context_steps\",\n",
    "            expression=\"\"\"\n",
    "                sum(\n",
    "                    reduce(\n",
    "                        sum(\n",
    "                            query(qt) * unpack_bits(attribute(colbert_steps)), v\n",
    "                        ),\n",
    "                        max, token\n",
    "                    ),\n",
    "                    querytoken\n",
    "                )\n",
    "            \"\"\",\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"avg_sim_steps\", expression=\"reduce(max_sim_per_context_steps, avg, steps)\"\n",
    "        ),\n",
    "    ],\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_steps\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"avg_sim_steps\"),\n",
    "    match_features=[\"cos_sim_steps\", \"avg_sim_steps\", \"max_sim_per_context_steps\"],\n",
    ")\n",
    "\n",
    "colbert_max_ingredients = RankProfile(\n",
    "    name=\"colbert_max_ingredients\",\n",
    "    inputs = [\n",
    "        (\"query(q)\", \"tensor<float>(x[384])\"),\n",
    "        (\"query(qt)\", \"tensor<float>(querytoken{}, v[128])\"),\n",
    "    ],\n",
    "    functions=[\n",
    "        Function(name=\"cos_sim_ingredients\", expression=\"closeness(field, embedding_ingredients)\"),\n",
    "        Function(\n",
    "            name=\"max_sim_per_context_ingredients\",\n",
    "            expression=\"\"\"\n",
    "                sum(\n",
    "                    reduce(\n",
    "                        sum(\n",
    "                            query(qt) * unpack_bits(attribute(colbert_ingredients)), v\n",
    "                        ),\n",
    "                        max, token\n",
    "                    ),\n",
    "                    querytoken\n",
    "                )\n",
    "            \"\"\",\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"max_sim_ingredients\", expression=\"reduce(max_sim_per_context_ingredients, max, ingredients)\"\n",
    "        ),\n",
    "    ],\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_ingredients\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"max_sim_ingredients\"),\n",
    "    match_features=[\"cos_sim_ingredients\", \"max_sim_ingredients\", \"max_sim_per_context_ingredients\"],\n",
    ")\n",
    "\n",
    "colbert_avg_ingredients = RankProfile(\n",
    "    name=\"colbert_avg_ingredients\",\n",
    "    inputs = [\n",
    "        (\"query(q)\", \"tensor<float>(x[384])\"),\n",
    "        (\"query(qt)\", \"tensor<float>(querytoken{}, v[128])\"),\n",
    "    ],\n",
    "    functions=[\n",
    "        Function(name=\"cos_sim_ingredients\", expression=\"closeness(field, embedding_ingredients)\"),\n",
    "        Function(\n",
    "            name=\"max_sim_per_context_ingredients\",\n",
    "            expression=\"\"\"\n",
    "                sum(\n",
    "                    reduce(\n",
    "                        sum(\n",
    "                            query(qt) * unpack_bits(attribute(colbert_ingredients)), v\n",
    "                        ),\n",
    "                        max, token\n",
    "                    ),\n",
    "                    querytoken\n",
    "                )\n",
    "            \"\"\",\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"avg_sim_ingredients\", expression=\"reduce(max_sim_per_context_ingredients, avg, ingredients)\"\n",
    "        ),\n",
    "    ],\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_ingredients\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"avg_sim_ingredients\"),\n",
    "    match_features=[\"cos_sim_ingredients\", \"avg_sim_ingredients\", \"max_sim_per_context_ingredients\"],\n",
    ")\n",
    "\n",
    "colbert_max_body_split_bm25 = RankProfile(\n",
    "    name=\"colbert_max_body_split_bm25\",\n",
    "    inherits=\"colbert_max_body_split\",\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_body_split\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"max_sim_body_split + bm25(title)\"),\n",
    ")\n",
    "\n",
    "colbert_avg_body_split_bm25 = RankProfile(\n",
    "    name=\"colbert_avg_body_split_bm25\",\n",
    "    inherits=\"colbert_avg_body_split\",\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_body_split\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"avg_sim_body_split + bm25(title)\"),\n",
    ")\n",
    "\n",
    "colbert_max_steps_bm25 = RankProfile(\n",
    "    name=\"colbert_max_steps_bm25\",\n",
    "    inherits=\"colbert_max_steps\",\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_steps\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"max_sim_steps + bm25(title)\"),\n",
    ")\n",
    "\n",
    "colbert_avg_steps_bm25 = RankProfile(\n",
    "    name=\"colbert_avg_steps_bm25\",\n",
    "    inherits=\"colbert_avg_steps\",\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_steps\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"avg_sim_steps + bm25(title)\"),\n",
    ")\n",
    "\n",
    "colbert_max_ingredients_bm25 = RankProfile(\n",
    "    name=\"colbert_max_ingredients_bm25\",\n",
    "    inherits=\"colbert_max_ingredients\",\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_ingredients\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"max_sim_ingredients + bm25(title)\"),\n",
    ")\n",
    "\n",
    "colbert_avg_ingredients_bm25 = RankProfile(\n",
    "    name=\"colbert_avg_ingredients_bm25\",\n",
    "    inherits=\"colbert_avg_ingredients\",\n",
    "    first_phase=FirstPhaseRanking(expression=\"cos_sim_ingredients\"),\n",
    "    second_phase=SecondPhaseRanking(expression=\"avg_sim_ingredients + bm25(title)\"),\n",
    ")\n",
    "\n",
    "\n",
    "app_package.schema.add_rank_profile(colbert_max_body_split)\n",
    "app_package.schema.add_rank_profile(colbert_avg_body_split)\n",
    "app_package.schema.add_rank_profile(colbert_max_steps)\n",
    "app_package.schema.add_rank_profile(colbert_avg_steps)\n",
    "app_package.schema.add_rank_profile(colbert_max_ingredients)\n",
    "app_package.schema.add_rank_profile(colbert_avg_ingredients)\n",
    "app_package.schema.add_rank_profile(colbert_max_body_split_bm25)\n",
    "app_package.schema.add_rank_profile(colbert_avg_body_split_bm25)\n",
    "app_package.schema.add_rank_profile(colbert_max_steps_bm25)\n",
    "app_package.schema.add_rank_profile(colbert_avg_steps_bm25)\n",
    "app_package.schema.add_rank_profile(colbert_max_ingredients_bm25)\n",
    "app_package.schema.add_rank_profile(colbert_avg_ingredients_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Path(\"pkg\").mkdir(parents=True, exist_ok=True)\n",
    "#app_package.to_files(\"pkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! mkdir -p pkg/model\n",
    "#! curl -L -o pkg/model/tokenizer.json \\\n",
    "#  https://huggingface.co/colbert-ir/colbertv2.0/raw/main/tokenizer.json\n",
    "\n",
    "#! curl -L -o pkg/model/model.onnx \\\n",
    "#  https://huggingface.co/colbert-ir/colbertv2.0/resolve/main/model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos realizar o *deploy* do pacote do Vespa pelo Docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for configuration server, 0/300 seconds...\n",
      "Waiting for configuration server, 5/300 seconds...\n",
      "Waiting for configuration server, 10/300 seconds...\n",
      "Waiting for configuration server, 15/300 seconds...\n",
      "Waiting for configuration server, 20/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 0/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 5/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 10/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 15/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 20/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 25/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 30/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 35/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 40/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 45/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 50/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 55/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 60/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 65/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 70/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 75/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 80/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 85/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 90/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 95/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 100/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 105/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 110/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 115/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 120/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 125/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 130/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 135/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 140/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Application is up!\n",
      "Finished deployment.\n"
     ]
    }
   ],
   "source": [
    "from vespa.deployment import VespaDocker\n",
    "\n",
    "#vespa_docker = VespaDocker()\n",
    "#app = vespa_docker.deploy_from_disk(application_name=\"findmypasta\", application_root=\"pkg\")\n",
    "\n",
    "vespa_docker = VespaDocker()\n",
    "app = vespa_docker.deploy(application_package=app_package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fornecendo dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma função que cria o campo `body`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipe_file_body_lines(recipe, complementary_data = None):\n",
    "    \"\"\"\n",
    "    Function responsible for creating the recipe body.\n",
    "    \"\"\"\n",
    "    # Transformar as colunas de strings para listas\n",
    "    recipe['tags'] = recipe['tags'].strip(\"[]\").replace(\"'\", \"\").split(', ')\n",
    "    recipe['steps'] = recipe['steps'].strip(\"[]\").replace(\"'\", \"\").split(', ')\n",
    "    recipe['ingredients'] = recipe['ingredients'].strip(\"[]\").replace(\"'\", \"\").split(', ')\n",
    "\n",
    "    # reviews = complementary_data[complementary_data['recipe_id'] == recipe['id']]\n",
    "\n",
    "    # # ordering by descending date\n",
    "    # reviews = reviews.sort_values('date', ascending=False)\n",
    "\n",
    "    # # getting the average rating\n",
    "    # avg_rating = reviews['rating'].mean()\n",
    "\n",
    "    # # if the average rating is NaN, we will set it to \"No reviews\"\n",
    "    # if np.isnan(avg_rating):\n",
    "    #     avg_rating = \"No reviews\"\n",
    "\n",
    "    # creating the recipe body\n",
    "    recipe_body = recipe['name'] + '\\n' \\\n",
    "    + \"Recipe posted on: \" + str(recipe['submitted']) + '\\n' \\\n",
    "    + \"Tags: \" + ', '.join(recipe['tags']) + '\\n' \\\n",
    "    + \"Description: \" + recipe['description'] + '\\n' \\\n",
    "    + \"This recipe takes \" + str(recipe['minutes']) + \" minutes to be done.\" + '\\n' \\\n",
    "    + \"For this recipe you will need the ingredients: \" + '\\n' \\\n",
    "    + ', '.join(recipe['ingredients']) + '\\n' \\\n",
    "    + \"The \" + str(recipe[\"n_steps\"]) + \" steps to make this recipe are: \" + '\\n' \\\n",
    "    + ', '.join(recipe['steps']) \n",
    "    return recipe_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar recipe_file_body_lines a cada linha do DataFrame de receitas\n",
    "def apply_recipe_file_body_lines(recipe_row):\n",
    "    return recipe_file_body_lines(recipe_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, pegamos os dados presentes no dataset `../input/RAW_recipes.csv`, definimos os campos que serão enviados e formatamos para o formato do Vespa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'id', 'minutes', 'contributor_id', 'submitted', 'tags',\n",
      "       'nutrition', 'n_steps', 'steps', 'description', 'ingredients',\n",
      "       'n_ingredients'],\n",
      "      dtype='object')\n",
      "{'put': 'id:recipes:findmypasta::137739', 'fields': {'id': 137739, 'title': 'arriba   baked winter squash mexican style', 'steps': ['make a choice and proceed with recipe', 'depending on size of squash , cut into half or fourths', 'remove seeds', 'for spicy squash , drizzle olive oil or melted butter over each cut squash piece', 'season with mexican seasoning mix ii', 'for sweet squash , drizzle melted honey , butter , grated piloncillo over each cut squash piece', 'season with sweet mexican spice mix', 'bake at 350 degrees , again depending on size , for 40 minutes up to an hour , until a fork can easily pierce the skin', 'be careful not to burn the squash especially if you opt to use sugar or butter', 'if you feel more comfortable , cover the squash with aluminum foil the first half hour , give or take , of baking', 'if desired , season with salt'], 'ingredients': ['winter squash', 'mexican seasoning', 'mixed spice', 'honey', 'butter', 'olive oil', 'salt'], 'body_split': ['arriba   baked winter squash mexican style', 'Recipe posted on: 2005-09-16', 'Tags: 60-minutes-or-less, time-to-make, course, main-ingredient, cuisine, preparation, occasion, north-american, side-dishes, vegetables, mexican, easy, fall, holiday-event, vegetarian, winter, dietary, christmas, seasonal, squash', 'Description: autumn is my favorite time of year to cook! this recipe \\r', 'can be prepared either spicy or sweet, your choice!\\r', 'two of my posted mexican-inspired seasoning mix recipes are offered as suggestions.', 'This recipe takes 55 minutes to be done.', 'For this recipe you will need the ingredients: ', 'winter squash, mexican seasoning, mixed spice, honey, butter, olive oil, salt', 'The 11 steps to make this recipe are: ', 'make a choice and proceed with recipe, depending on size of squash , cut into half or fourths, remove seeds, for spicy squash , drizzle olive oil or melted butter over each cut squash piece, season with mexican seasoning mix ii, for sweet squash , drizzle melted honey , butter , grated piloncillo over each cut squash piece, season with sweet mexican spice mix, bake at 350 degrees , again depending on size , for 40 minutes up to an hour , until a fork can easily pierce the skin, be careful not to burn the squash especially if you opt to use sugar or butter, if you feel more comfortable , cover the squash with aluminum foil the first half hour , give or take , of baking, if desired , season with salt']}}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../input/RAW_recipes.csv')\n",
    "###\n",
    "##\n",
    "\n",
    "# print columns\n",
    "print(df.columns)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df['body'] = df.apply(apply_recipe_file_body_lines, axis=1)\n",
    "df['body_split'] = df['body'].str.split('\\n')\n",
    "\n",
    "df['minutes'] = \"This recipe takes \" + df['minutes'].astype(str) + \" minutes to be done.\"\n",
    "df['submitted'] = 'Recipe submitted on: ' + df[\"submitted\"]\n",
    "df['tags'] = df[\"tags\"]\n",
    "df['n_steps'] = 'Number of steps to make this recipe: ' + df['n_steps'].astype(str)\n",
    "df['n_ingredients'] = 'Number of ingredients: ' + df['n_ingredients'].astype(str)\n",
    "df['steps'] = df[\"steps\"]\n",
    "df['description'] = df[\"description\"]\n",
    "df['ingredients'] = df[\"ingredients\"]\n",
    "df['title'] = df['name']\n",
    "\n",
    "namespace = \"recipes\"\n",
    "document_type = \"findmypasta\"\n",
    "\n",
    "def to_vespa_format(x):\n",
    "    document_id = f\"id:{namespace}:{document_type}::{x['id']}\"\n",
    "    return {\n",
    "        \"put\": document_id,\n",
    "        \"fields\": {\n",
    "            \"id\": x[\"id\"],\n",
    "            \"title\": x[\"name\"],\n",
    "            #\"tags\": ast.literal_eval(x[\"tags\"]),\n",
    "            \"steps\": ast.literal_eval(x[\"steps\"]),\n",
    "            #\"description\": x[\"description\"],\n",
    "            \"ingredients\": ast.literal_eval(x[\"ingredients\"]),\n",
    "            #\"minutes\": x[\"minutes\"],\n",
    "            #\"n_steps\": x[\"n_steps\"],\n",
    "            #\"n_ingredients\": x[\"n_ingredients\"],\n",
    "            #\"submitted\": x[\"submitted\"],\n",
    "            #\"body\": x[\"body\"],\n",
    "            \"body_split\": x[\"body_split\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "vespa_feed = df.apply(to_vespa_format, axis=1).tolist()\n",
    "vespa_feed_slice = vespa_feed[0:100]\n",
    "print(vespa_feed_slice[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria o json com os campos formatados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vespa_feed2.jsonl\", \"w\") as f:\n",
    "    for item in vespa_feed_slice:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alimenta o Vespa com os documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"feeder.operation.count\": 100,\n",
      "  \"feeder.seconds\": 134.560,\n",
      "  \"feeder.ok.count\": 100,\n",
      "  \"feeder.ok.rate\": 0.743,\n",
      "  \"feeder.error.count\": 0,\n",
      "  \"feeder.inflight.count\": 0,\n",
      "  \"http.request.count\": 100,\n",
      "  \"http.request.bytes\": 81098,\n",
      "  \"http.request.MBps\": 0.001,\n",
      "  \"http.exception.count\": 0,\n",
      "  \"http.response.count\": 100,\n",
      "  \"http.response.bytes\": 9422,\n",
      "  \"http.response.MBps\": 0.000,\n",
      "  \"http.response.error.count\": 0,\n",
      "  \"http.response.latency.millis.min\": 12402,\n",
      "  \"http.response.latency.millis.avg\": 21720,\n",
      "  \"http.response.latency.millis.max\": 44415,\n",
      "  \"http.response.code.counts\": {\n",
      "    \"200\": 100\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! vespa config set target local\n",
    "! vespa feed vespa_feed2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = app.query(yql = \"select * from sources * where true\")\n",
    "documents.number_documents_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "Vamos pegar as queries que serão feitas, presentes no dataset `../input/Recipe_Search_Questions.xlsx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Descrição</th>\n",
       "      <th>Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Keywords</td>\n",
       "      <td>Pergunta simples</td>\n",
       "      <td>grilled cheese sandwich recipe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keywords</td>\n",
       "      <td>Pergunta simples</td>\n",
       "      <td>mango smoothie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semantica</td>\n",
       "      <td>Pergunta média</td>\n",
       "      <td>gluten-free bread without yeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Semantica</td>\n",
       "      <td>Pergunta média</td>\n",
       "      <td>low carb dessert for diabetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Semantica</td>\n",
       "      <td>Pergunta difícil</td>\n",
       "      <td>traditional Japanese breakfast for a family</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Tipo         Descrição                                        Query\n",
       "0   Keywords  Pergunta simples               grilled cheese sandwich recipe\n",
       "1   Keywords  Pergunta simples                               mango smoothie\n",
       "2  Semantica    Pergunta média              gluten-free bread without yeast\n",
       "3  Semantica    Pergunta média               low carb dessert for diabetics\n",
       "4  Semantica  Pergunta difícil  traditional Japanese breakfast for a family"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the Questions.xlsx and answering each question query\n",
    "import pandas as pd\n",
    "questions = pd.read_excel('../input/Questions.xlsx')\n",
    "questions = pd.read_excel('../input/Recipe_Search_Questions.xlsx')\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geramos o arquivo de output com as respostas para cada *query*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.io import VespaQueryResponse\n",
    "import json\n",
    "\n",
    "# Supondo que 'questions' é um DataFrame com colunas ['Query', 'Tipo', 'Descrição']\n",
    "data = pd.DataFrame(columns=['id', 'title', 'Query', 'Tipo', 'Descrição'])\n",
    "\n",
    "model_to_ranking_dict = {\n",
    "    \"colbert_max_body_split\": \"colbert_max_body_split\",\n",
    "    \"colbert_avg_body_split\": \"colbert_avg_body_split\",\n",
    "    \"colbert_max_steps\": \"colbert_max_steps\",\n",
    "    \"colbert_avg_steps\": \"colbert_avg_steps\",\n",
    "    \"colbert_max_ingredients\": \"colbert_max_ingredients\",\n",
    "    \"colbert_avg_ingredients\": \"colbert_avg_ingredients\",\n",
    "    \"colbert_max_body_split_bm25\": \"colbert_max_body_split_bm25\",\n",
    "    \"colbert_avg_body_split_bm25\": \"colbert_avg_body_split_bm25\",\n",
    "    \"colbert_max_steps_bm25\": \"colbert_max_steps_bm25\",\n",
    "    \"colbert_avg_steps_bm25\": \"colbert_avg_steps_bm25\",\n",
    "    \"colbert_max_ingredients_bm25\": \"colbert_max_ingredients_bm25\",\n",
    "    \"colbert_avg_ingredients_bm25\": \"colbert_avg_ingredients_bm25\",\n",
    "}\n",
    "\n",
    "embeddings = {\n",
    "    \"colbert_max_body_split\": \"embedding_body_split\",\n",
    "    \"colbert_avg_body_split\": \"embedding_body_split\",\n",
    "    \"colbert_max_steps\": \"embedding_steps\",\n",
    "    \"colbert_avg_steps\": \"embedding_steps\",\n",
    "    \"colbert_max_ingredients\": \"embedding_ingredients\",\n",
    "    \"colbert_avg_ingredients\": \"embedding_ingredients\",\n",
    "    \"colbert_max_body_split_bm25\": \"embedding_body_split\",\n",
    "    \"colbert_avg_body_split_bm25\": \"embedding_body_split\",\n",
    "    \"colbert_max_steps_bm25\": \"embedding_steps\",\n",
    "    \"colbert_avg_steps_bm25\": \"embedding_steps\",\n",
    "    \"colbert_max_ingredients_bm25\": \"embedding_ingredients\",\n",
    "    \"colbert_avg_ingredients_bm25\": \"embedding_ingredients\"\n",
    "}\n",
    "\n",
    "for selected_model in model_to_ranking_dict.keys():\n",
    "    output_name = 'output/Results_' + selected_model + '_extraQuestions' + '.xlsx'\n",
    "    embedding = embeddings[selected_model]\n",
    "\n",
    "    if model_to_ranking_dict[selected_model] is not None:\n",
    "        i = 0\n",
    "        for input_query in questions['Query']:\n",
    "            # save a checkpoint each 100 queries\n",
    "            if i % 100 == 0:\n",
    "                data.to_excel(output_name, index=False)\n",
    "\n",
    "            with app.syncio(connections=1) as session:\n",
    "                try:\n",
    "                    response: VespaQueryResponse = session.query(\n",
    "                        yql=\"select * from sources * where ({targetHits:1000}nearestNeighbor(\" + embedding + \",q)) limit 5\",\n",
    "                        query=input_query,\n",
    "                        ranking=model_to_ranking_dict[selected_model],\n",
    "                        body={\n",
    "                            \"input.query(q)\": f\"embed(e5, \\\"{input_query}\\\")\",\n",
    "                            \"input.query(qt)\": f\"embed(colbert, \\\"{input_query}\\\")\",\n",
    "                            # \"input.query(q)\": f\"embed({input_query})\",\n",
    "                            #\"timeout\": \"30s\"  # Aumentar o tempo limite para 10 segundos\n",
    "                        }\n",
    "                    )\n",
    "                    assert response.is_successful()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with query '{input_query}': {e}\")\n",
    "                    continue\n",
    "\n",
    "                for hit in response.hits:\n",
    "                    record = {}\n",
    "                    for field in ['id', 'title']:\n",
    "                        record[field] = hit['fields'].get(field, None)\n",
    "                    record[\"Query\"] = input_query\n",
    "                    record[\"Tipo\"] = questions[questions['Query'] == input_query]['Tipo'].values[0]\n",
    "                    record[\"Descrição\"] = questions[questions['Query'] == input_query]['Descrição'].values[0]\n",
    "                    data = pd.concat([data, pd.DataFrame([record])], ignore_index=True)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        # Sorting\n",
    "        data = data.sort_values(by=['Tipo', 'Query'])\n",
    "\n",
    "        # reordering columns\n",
    "        data = data[['Tipo', 'Descrição', 'Query', 'id', 'title']]\n",
    "\n",
    "        # exporting to excel\n",
    "        data.to_excel(output_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respostas para a query `chocolate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 58651, 'title': 'turtle  squares', 'body_split': ['turtle  squares', 'Recipe posted on: 2003-04-07', 'Tags: 30-minutes-or-less, time-to-make, course, main-ingredient, cuisine, preparation, occasion, north-american, for-large-groups, desserts, fruit, oven, easy, finger-food, kid-friendly, cookies-and-brownies, chocolate, bar-cookies, nuts, dietary, low-sodium, low-in-something, taste-mood, sweet, equipment, number-of-servings, presentation', 'Description: for lovers of pecans and chocolate...', 'This recipe takes 30 minutes to be done.', 'For this recipe you will need the ingredients: ', 'flour, brown sugar, butter, pecans, semi-sweet chocolate chips', 'The 15 steps to make this recipe are: ', 'preheat oven to 350 degrees f, spray a 13 x 9 baking pan evenly with non-stick cooking spray, beat 1 cup brown sugar with 1 / 2 cup melted butter with an electric mixer on medium for 2-3 minutes, add the flour mixture and mix until smooth, press the flour mixture evenly and firmly into the prepared baking pan, sprinkle the pecans evenly oven the flour mixture in the pan, mix the 2 / 3 cup butter and 1 / 2 cup brown sugar in a saucepan and bring to a boil over a medium heat, stir constantly while boiling for about 1 minute, spread the boiling mixture evenly over the pecans, bake for about 20 minutes, cool, melt the chocolate chips in the microwave , stirring ever 30 seconds or so until almost smooth, remove the chocolate chips , stir thoroughly to smooth and spread evenly over the top of the squares, chill for 15 minutes until the chocolate sets and then remove from fridge, when squares have returned to room temperature cut into 36 squares']}\n",
      "{'id': 32271, 'title': 'one pot  brownies', 'body_split': ['one pot  brownies', 'Recipe posted on: 2002-06-25', 'Tags: 60-minutes-or-less, time-to-make, course, main-ingredient, preparation, desserts, easy, cookies-and-brownies, chocolate, bar-cookies, brownies, dietary', \"Description: this recipe was submitted by mrs. stuart padnos for a cookbook fund raiser put out by members of the junior welfare league, holland, michigan about 30 or so years ago. my mom brought the cookbook back to me as a souvenir of their trip. i've made these brownies all these years and have great luck with them every time. i like to top them with an easy cooked chocolate fudge icing that i got from a bowling partner several years ago. these brownies are like a chocolate cake-not gooey like some recipes and i do personally like them plain without nuts or raisins.\", 'This recipe takes 40 minutes to be done.', 'For this recipe you will need the ingredients: ', 'unsweetened chocolate squares, butter, sugar, eggs, flour, vanilla, nuts', 'The 8 steps to make this recipe are: ', 'melt chocolate and butter in heavy pot, let cool slightly , then add sugar and mix well, add eggs , mix well , then beat in flour, add vanilla, add nuts or raisins , if desired, stir well, bake in 9 1 / 2 x 13 inch greased pan for 20-25 minutes at 375 degrees, do not overbake']}\n",
      "{'id': 27087, 'title': 'get the sensation  brownies', 'body_split': ['get the sensation  brownies', 'Recipe posted on: 2002-05-03', 'Tags: weeknight, time-to-make, course, main-ingredient, preparation, occasion, desserts, oven, easy, potluck, holiday-event, kid-friendly, picnic, vegetarian, cookies-and-brownies, chocolate, bar-cookies, brownies, dietary, gifts, comfort-food, taste-mood, to-go, equipment, number-of-servings, 4-hours-or-less', 'Description: a yummy thick chocolate mint brownie from york peppermint patties! get the sensation!', 'This recipe takes 70 minutes to be done.', 'For this recipe you will need the ingredients: ', 'butter, sugar, vanilla, eggs, all-purpose flour, baking cocoa, baking powder, salt, miniature peppermint patties', 'The 11 steps to make this recipe are: ', 'preheat oven to 350 degrees, grease 13 x 9 baking pan, in a large bowl , whisk together butter , sugar and vanilla, add eggs and stir until well combined, combine dry ingredients and blend well with wet ingredients, reserve 2 cups of batter and set aside, spread remaining batter in prepared pan, arrange peppermint patties in a single layer over batter , about 1 / 2 inch apart, carefully spread reserved 2 cups of batter on top, bake for 50- 55 minutes or until brownies begin to pull away from sides of pan, cool completely on wire rack and cut into squares']}\n",
      "{'id': 62368, 'title': 'the best  chocolate chip cheesecake ever', 'body_split': ['the best  chocolate chip cheesecake ever', 'Recipe posted on: 2003-05-17', 'Tags: time-to-make, course, preparation, desserts, cheesecake, number-of-servings, 4-hours-or-less', \"Description: i wrote this recipe down once while visiting a girl friend(she just had it on an index card) i hadn't even tried it and boy was i not sorry...this is the best! i get rave reviews whenever i make this. my sister in law has actually brought me all the ingredients on several occasions to make this for her to take to work, etc.. if you like cheesecake i do not think this will disappoint!\", 'This recipe takes 90 minutes to be done.', 'For this recipe you will need the ingredients: ', 'oreo cookie crumbs, butter, cream cheese, sweetened condensed milk, eggs, vanilla, chocolate chips, flour, sour cream, brown sugar', 'The 12 steps to make this recipe are: ', 'preheat oven to 300, combine cookie crumbs& butter , press firmly on bottom of 9 inch springform pan, in large mixing bowl beat cream cheese until fluffy , beat in sweetened condensed milk , eggs& vanilla, in small bowl toss 1 / 2 cup of chocolate chips with flour , stir into cheesecake mixture, pour into prepared pan, sprinkle remaining chips on top, bake 1 hour , cool& chill, for topping beat one cup sour cream& 2tbsp, brown sugar , pour over cake , cook additional 10 mins, dripmelted semi-sweet chocolate over top, i usually decorate with fresh strawberries and serve slices with spray whipped cream, awesome !']}\n",
      "{'id': 35964, 'title': 'rich  hot fudge cake', 'body_split': ['rich  hot fudge cake', 'Recipe posted on: 2002-08-02', 'Tags: 60-minutes-or-less, time-to-make, course, main-ingredient, preparation, occasion, low-protein, healthy, desserts, oven, easy, cakes, chocolate, dietary, low-sodium, low-cholesterol, low-saturated-fat, comfort-food, healthy-2, low-in-something, taste-mood, equipment', \"Description: this is a very rich hot fudge cake that i got hooked on when i was expecting our daughter - just couldn't get enough of it and now she loves chocolate....hum... anyway, this is a easy recipe that i love warm with ice cream and my husband loves cold and plain. happy gobbling!:o)\", 'This recipe takes 45 minutes to be done.', 'For this recipe you will need the ingredients: ', 'flour, sugar, cocoa, baking powder, salt, milk, vegetable oil, vanilla, brown sugar, hot water', 'The 10 steps to make this recipe are: ', 'combine flour , sugar , 2 t cocoa , baking powder , and salt, stir in milk , oil , and vanilla until smooth, spread in an ungreased 9\" square pan, combine brown sugar and remaining 4 t cocoa, sprinkle over batter, pour hot water over all, do not stir, bake@ 350 for 35-40 min, serve warm or cold with vanilla ice cream, enjoy !']}\n",
      "{'id': 71635, 'title': 'no bake  cookie crumble cheesecake', 'body_split': ['no bake  cookie crumble cheesecake', 'Recipe posted on: 2003-09-26', 'Tags: weeknight, time-to-make, course, preparation, desserts, cheesecake, 4-hours-or-less', 'Description: yet another one i have not made, but have had for awhile. sounds good and very versatile as you choose what kind of cookies to use! also sounds extremely easy(not to mention quick) as you whip this all up in a blender and put into a storebought crust!! most of my cheesecake making has been baked ones so i hope this ', 'This recipe takes 135 minutes to be done.', 'For this recipe you will need the ingredients: ', 'gelatin, milk, cream cheese, sugar, vanilla extract, miniature semisweet chocolate chips, prepared graham cracker crusts, cookie', 'The 8 steps to make this recipe are: ', 'in blender , sprinkle gelatin over cold milk, let stand 2 minutes, add hot milk and process at low until dissolved , about 2 minutes, add cream cheese , sugar and vanilla and process until blended, arrange mini chocolate chips in the bottom of the crust, pour in gelatin mixture, sprinkle with your favourite crushed cookies, chill until firm , about 2 hours']}\n",
      "{'id': 44895, 'title': 'symphony  brownies', 'body_split': ['symphony  brownies', 'Recipe posted on: 2002-10-29', 'Tags: 60-minutes-or-less, time-to-make, course, preparation, 5-ingredients-or-less, desserts, easy, cookies-and-brownies, bar-cookies, brownies, number-of-servings', 'Description: a ', 'This recipe takes 45 minutes to be done.', 'For this recipe you will need the ingredients: ', 'betty crocker fudge brownie mix, eggs, water, vegetable oil, milk chocolate candy bars', 'The 10 steps to make this recipe are: ', 'preheat oven to 350 degrees , or 325 degrees for glass baking dish, lightly grease the bottom only of an 11 x 7 inch baking pan, set aside, in a large mixing bowl prepare brownie mix according to directions on the package , using 2 eggs , 1 / 4 cup water and 1 / 2 cup oil, stir until smooth, spread half the batter evenly into prepared pan, unwrap candy bars and place them on top of the batter, top with remaining brownie batter, bake 40 minutes or until a toothpick inserted near center comes out with only a few moist crumbs clinging to it, remove pan from oven and let cool on wire rack before cutting into squares or other shapes']}\n",
      "{'id': 39363, 'title': 'the best  banana bread  or muffins', 'body_split': ['the best  banana bread  or muffins', 'Recipe posted on: 2002-09-03', 'Tags: weeknight, time-to-make, course, main-ingredient, preparation, occasion, for-large-groups, breads, fruit, oven, holiday-event, kid-friendly, muffins, dietary, comfort-food, quick-breads, tropical-fruit, bananas, taste-mood, equipment, number-of-servings, 4-hours-or-less', \"Description: okay, another banana bread recipe...but maybe the other ones don't have at least one ingredient that this one does...my family loves the muffins from this recipe...especially when i make mini-muffins! (i usually halve the recipe!) makes 4 loaves or 60 muffins\", 'This recipe takes 70 minutes to be done.', 'For this recipe you will need the ingredients: ', 'butter, sugar, eggs, bananas, water, baking soda, salt, baking powder, flour, nuts, chocolate chips', 'The 10 steps to make this recipe are: ', 'cream together butter and sugar, blend in eggs, add bananas , water , soda , salt , and baking powder, blend well, stir in flour, add nuts and / or chips , if desired, pour into 4 greased / floured loaf pans or lined muffin tins, bake at 350: loaves need 35-40 minutes , or until toothpick inserted in center comes out clean, muffins need only 18-20 minutes, with this recipe , i usually always make muffins to cut down bake time']}\n",
      "{'id': 23933, 'title': 'chinese  candy', 'body_split': ['chinese  candy', 'Recipe posted on: 2002-03-29', 'Tags: 15-minutes-or-less, time-to-make, course, preparation, occasion, 5-ingredients-or-less, desserts, easy, kid-friendly, candy, dietary, number-of-servings', 'Description: a little different, and oh so good. i include these when making up candy trays, as gifts, at christmas time.', 'This recipe takes 15 minutes to be done.', 'For this recipe you will need the ingredients: ', 'butterscotch chips, chinese noodles, salted peanuts', 'The 4 steps to make this recipe are: ', 'melt butterscotch chips in heavy saucepan over low heat, fold in peanuts and chinese noodles until coated, drop by tablespoon onto waxed paper, let stand in cool place until firm']}\n",
      "{'id': 107699, 'title': 'deep fried dessert thingys', 'body_split': ['deep fried dessert thingys', 'Recipe posted on: 2005-01-05', 'Tags: 30-minutes-or-less, time-to-make, course, preparation, occasion, low-protein, desserts, deep-fry, stove-top, dietary, high-calcium, high-in-something, low-in-something, taste-mood, sweet, equipment, technique', \"Description: my mother used to make this for us as a special treat. i don't know where she got this recipe or what it is called (hence the title). my husband loves this and encouraged me to submit it. this recipe is full of sugar and oil and is as unhealthy as a desert can be but very yummy.\", 'This recipe takes 20 minutes to be done.', 'For this recipe you will need the ingredients: ', 'all-purpose flour, granulated sugar, baking powder, salt, vanilla extract, egg, milk, vegetable oil, bread, brown sugar, ground cinnamon, butter, powdered sugar', 'The 20 steps to make this recipe are: ', 'in a large bowl , mix flour , granulated sugar , baking powder and salt, make a well in the center of the flour mixture , and pour in milk , vanilla , egg and 2 tablespoons oil, mix until smooth, heat remaining vegetable oil to 400 degrees in a small deep fryer, oil is hot enough when it starts to ripple, be careful not to overheat, butter one side of each bread slice, sprinkle the brown sugar and cinnamon evenly onto the buttered side of the bread slices, make sandwiches by placing the slices of bread together with the brown sugar on the inside, cut the sandwiches into quarters, now is a good time to test the oil by dropping in a very small amount of batter, batter should sizzle and brown in about 20 seconds, oil needs to be hot enough to melt the brown sugar inside the sandwiches but not so hot that it scorches the batter, dip the sandwiches into the batter mixture, using deep fryer tongs , carefully place the battered sandwiches into the heated oil, be cautious when deep frying, try to avoid burns from splattering hot oil, remove sandwiches from the oil when they are golden brown, set aside to cool for a couple of minutes, sprinkle with powdered sugar and serve']}\n"
     ]
    }
   ],
   "source": [
    "from vespa.io import VespaQueryResponse\n",
    "\n",
    "with app.syncio(connections=1) as session:\n",
    "    response:VespaQueryResponse = session.query(\n",
    "        yql=\"select * from sources * where ({targetHits:1000}nearestNeighbor(embedding_body_split,q))\",\n",
    "        ranking=\"colbert_max_body_split\",\n",
    "        query=\"chocolate\", \n",
    "        body={\n",
    "            \"input.query(q)\": f'embed(e5, \"chocolate\")',\n",
    "            \"input.query(qt)\": f'embed(colbert, \"chocolate\")',\n",
    "        },\n",
    "    )\n",
    "\n",
    "assert(response.is_successful())\n",
    "for hit in response.hits:\n",
    "    record = {}\n",
    "    for field in ['id', 'title', 'body_split']:\n",
    "        record[field] = hit['fields'][field]\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver a relevância total e média de cada canto, sendo que:\n",
    "\n",
    "- `0`:  \"recipe_body = recipe['name'] + '\\n'\";\n",
    "- `1`:  \"Recipe posted on: \" + str(recipe['submitted']) + '\\n'\";\n",
    "- `2`:  \"Tags: \" + ', '.join(recipe['tags']) + '\\n'\";\n",
    "- `3`:  \"Description: \" + recipe['description'] + '\\n'\";\n",
    "- `4`:  \"This recipe takes \" + str(recipe['minutes']) + \" minutes to be done.\" + '\\n'\";\n",
    "- `5`:  \"For this recipe you will need the ingredients: \" + '\\n'\";\n",
    "- `6`:  ', '.join(recipe['ingredients']) + '\\n'\";\n",
    "- `7`:  \"The \" + str(recipe[\"n_steps\"]) + \" steps to make this recipe are: \" + '\\n'\";\n",
    "- `8`:  ', '.join(recipe['steps'])\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 420.0016670227051, '1': 274.50904846191406, '2': 308.63941764831543, '3': 416.2218737602234, '4': 181.57076835632324, '5': 84.61458206176758, '6': 570.7690467834473, '7': 135.15281772613525, '8': 338.6511507034302}\n",
      "{'0': 42.000166702270505, '1': 27.450904846191406, '2': 30.863941764831544, '3': 41.62218737602234, '4': 18.157076835632324, '5': 8.461458206176758, '6': 57.076904678344725, '7': 13.515281772613525, '8': 33.865115070343016}\n"
     ]
    }
   ],
   "source": [
    "#response.hits[0]\n",
    "# get the cells of the first hit\n",
    "#print(response.hits[0]['fields']['matchfeatures']['max_sim_per_context']['cells'])\n",
    "\n",
    "total = {'0': 0, '1': 0, '2': 0, '3': 0, '4': 0, '5': 0, '6': 0, '7': 0, '8': 0}\n",
    "\n",
    "for hit in response.hits:\n",
    "    cells = hit['fields']['matchfeatures']['max_sim_per_context_body_split']['cells']\n",
    "    for key in total.keys():\n",
    "        total[key] += cells[key]\n",
    "\n",
    "median = total \n",
    "median = {k: v / len(response.hits) for k, v in median.items()}\n",
    "\n",
    "print(total)\n",
    "print(median)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
