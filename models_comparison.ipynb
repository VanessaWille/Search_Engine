{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal is to compare different models\n",
    "\n",
    "For this task we already have a set of queries that have been evaluated, we will load them and use to calculate the scores for the models search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# trying to load the review bank\n",
    "try:\n",
    "    review_bank = pd.read_excel('reviews/review_bank.xlsx')\n",
    "except:\n",
    "    review_bank = pd.DataFrame()\n",
    "\n",
    "# trying to load the missing reviews bank\n",
    "try:\n",
    "    missing_reviews_bank = pd.read_excel(\"reviews/missing_reviews_bank.xlsx\")\n",
    "    extra_reviews = missing_reviews_bank[missing_reviews_bank[\"Nota\"] != None]\n",
    "    extra_reviews.rename(columns={\"title\": \"Receita\"}, inplace=True)\n",
    "    extra_reviews[[\"Tipo\", \"Descrição\", \"Query\", \"Receita\", \"Evaluator\", \"Nota\"]]\n",
    "    review_bank = pd.concat([review_bank, extra_reviews])\n",
    "except:\n",
    "    missing_reviews_bank = pd.DataFrame()\n",
    "\n",
    "\n",
    "# the function for getting the ratings for pre-evaluated query-recipe pairs\n",
    "def lookup_rating(query, recipe):\n",
    "    try:\n",
    "        ratings = review_bank[(review_bank['Query'] == query) & (review_bank['Receita'] == recipe)][[\"Nota\", \"Evaluator\"]]\n",
    "        person_rating = ratings[ratings['Evaluator'] == \"Person\"]\n",
    "        if not person_rating.empty:\n",
    "            # If there is a human evaluation, it gets the preference\n",
    "            return person_rating.values[0][0]\n",
    "        else:\n",
    "            nota = ratings.iloc[0].values[0]\n",
    "            return nota\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the files in the output folder that are in the format Results_*.xlsx\n",
    "pattern = r\"Results_.*\\.xlsx$\"\n",
    "\n",
    "model_results_paths = [os.path.join('output', file) for file in os.listdir('output') if re.match(pattern, file)]\n",
    "\n",
    "models = {}\n",
    "for model_result_path in model_results_paths:\n",
    "    model_name = re.search(r\"Results_(.*).xlsx\", os.path.basename(model_result_path)).group(1)\n",
    "\n",
    "    result_df = pd.read_excel(model_result_path)\n",
    "    result_df[\"Nota\"] = result_df.apply(lambda row: lookup_rating(row['Query'], row['title']), axis=1)\n",
    "\n",
    "    models[model_name] = result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Bm25\n",
      "Avaliações ausentes: 0\n",
      "Média de pontuação: 2.6\n",
      "\n",
      "Modelo: bm25_extraQuestions\n",
      "Avaliações ausentes: 0\n",
      "Média de pontuação: 1.9560975609756097\n",
      "\n",
      "Modelo: hybrid\n",
      "Avaliações ausentes: 0\n",
      "Média de pontuação: 3.327272727272727\n",
      "\n",
      "Modelo: hybrid_extraQuestions\n",
      "Avaliações ausentes: 0\n",
      "Média de pontuação: 3.424390243902439\n",
      "\n",
      "Modelo: semantic\n",
      "Avaliações ausentes: 0\n",
      "Média de pontuação: 3.3454545454545452\n",
      "\n",
      "Modelo: semantic_extraQuestions\n",
      "Avaliações ausentes: 0\n",
      "Média de pontuação: 3.424390243902439\n",
      "\n",
      "Modelo: Tfidf\n",
      "Avaliações ausentes: 0\n",
      "Média de pontuação: 2.6363636363636362\n",
      "\n",
      "Modelo: tfidf_extraQuestions\n",
      "Avaliações ausentes: 0\n",
      "Média de pontuação: 2.6731707317073172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_reviews = pd.DataFrame()\n",
    "\n",
    "for model in models:\n",
    "    df = models[model]\n",
    "    # Filtrar as linhas onde Nota é None\n",
    "    model_missing_reviews = df[df['Nota'].isnull()]\n",
    "    \n",
    "    # Calcular a média de Nota\n",
    "    mean_score = df['Nota'].mean()\n",
    "    \n",
    "    # Imprimir o relatório\n",
    "    print(f'Modelo: {model}')\n",
    "    print(f'Avaliações ausentes: {len(model_missing_reviews)}')\n",
    "    print(f'Média de pontuação: {mean_score}\\n')\n",
    "\n",
    "    missing_reviews = pd.concat([missing_reviews, model_missing_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Descrição</th>\n",
       "      <th>Query</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>Nota</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Tipo, Descrição, Query, id, title, body, Nota]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for chatbot evaluation\n",
    "def get_gpt_template(resulting_recipe, query):\n",
    "    return f\"\"\"\n",
    "        You are a query result validator assistant tasked with evaluating the quality of a given recipe in answering a given query.\n",
    "\n",
    "        Here is the recipe: \\\n",
    "\n",
    "        {resulting_recipe}\n",
    "\n",
    "        Here is the query:\\\n",
    "\n",
    "        {query}\n",
    "\n",
    "        Now evaluate from 0 to 5, the relevance of the recipe for answering the given query, where 0 is unrelated, 1 is poorly related, 2 is a little relevant but miss some important things, 3 is relevant but miss some restrictions, 4 is a relevant recipe that nearly matches all the possible criterias, and 5 is a perfect result, where every possible consideration and restrain included in the query is answered in the recipe. Include both your grading and a brief justificative of the grade.\n",
    "\n",
    "\n",
    "        Assistant grading:\n",
    "        Justificative:\n",
    "    \"\"\"\n",
    "\n",
    "missing_reviews[\"gpt_template\"] = missing_reviews.apply(lambda row: get_gpt_template(row['body'], row['Query']), axis=1)\n",
    "missing_reviews[\"Nota\"] = None\n",
    "missing_reviews[\"Evaluator\"] = \"GPT\"\n",
    "\n",
    "missing_reviews_bank = pd.concat([missing_reviews_bank, missing_reviews])\n",
    "\n",
    "missing_reviews_bank.drop_duplicates(inplace=True)\n",
    "\n",
    "# export to xlsx\n",
    "missing_reviews_bank.to_excel('reviews/missing_reviews_bank.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All reviews were evaluated!\n"
     ]
    }
   ],
   "source": [
    "# checks if there is any missing review\n",
    "if missing_reviews.empty:\n",
    "    print(\"All reviews were evaluated!\")\n",
    "else:\n",
    "    print(f\"{len(missing_reviews)} reviews are missing.\")\n",
    "\n",
    "    from fill_in_missing_reviews import fill_in_missing_reviews\n",
    "    fill_in_missing_reviews()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
